name: publish_pages

on:
  push:
    branches: [ main, dev ]
  workflow_dispatch:
    inputs:
      clean_build:
        description: 'Remove restored assets and rebuild everything from scratch'
        type: boolean
        default: false

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: install deps
        run: |
          sudo apt-get update
          sudo apt-get install -y jq zip curl wget

      - name: setup java
        uses: actions/setup-java@v5
        with:
          distribution: 'zulu'
          java-version: '21'

      - name: compute base url
        id: base
        run: |
          OWNER="${GITHUB_REPOSITORY_OWNER}"
          REPO="$(echo "${GITHUB_REPOSITORY#*/}")"
          echo "url=https://${OWNER}.github.io/${REPO}" >> $GITHUB_OUTPUT

      - name: set clean build flag
        id: clean
        run: |
          if [[ "${GITHUB_EVENT_NAME}" == "workflow_dispatch" && "${CLEAN_BUILD_INPUT}" == "true" ]]; then
            echo "value=true" >> "$GITHUB_OUTPUT"
            echo "üßπ Clean build requested. Restored assets will be skipped."
          else
            echo "value=false" >> "$GITHUB_OUTPUT"
          fi
        env:
          CLEAN_BUILD_INPUT: ${{ github.event.inputs.clean_build || 'false' }}

      - name: clean dist directory
        if: steps.clean.outputs.value == 'true'
        run: |
          echo "üßΩ Removing existing dist directory for clean build"
          rm -rf dist

      - name: restore old assets
        if: steps.clean.outputs.value != 'true'
        continue-on-error: true
        run: |
          # Download existing ZIP files from GitHub Pages to preserve old versions
          BASE_URL="https://${GITHUB_REPOSITORY_OWNER}.github.io/$(echo ${GITHUB_REPOSITORY#*/})"
          echo "üì• Downloading existing assets from: $BASE_URL"
          mkdir -p dist

          # Try to download stores.json to get list of all stores
          if curl -f -s "$BASE_URL/stores.json" -o /tmp/old_stores.json 2>/dev/null; then
            echo "‚úÖ Found existing stores.json"

            # Read each store index and download ZIPs and images from it
            for store_index in $(jq -r '.stores[].index' /tmp/old_stores.json | sed 's|.*/||'); do
              echo "üìÇ Processing store: $store_index"

              if curl -f -s "$BASE_URL/$store_index" -o "/tmp/$store_index" 2>/dev/null; then
                # Download ZIPs (only if they don't exist locally)
                jq -r '.items[]?.zip_url // empty' "/tmp/$store_index" | while read zip_url; do
                  if [[ -n "$zip_url" ]]; then
                    zip_path=$(echo "$zip_url" | sed "s|$BASE_URL/||")
                    if [[ -f "dist/$zip_path" ]]; then
                      echo "  ‚úÖ ZIP: $zip_path (already exists, skipping)"
                    else
                      mkdir -p "dist/$(dirname "$zip_path")"
                      echo "  ‚¨áÔ∏è  ZIP: $zip_path"
                      curl -f -s "$zip_url" -o "dist/$zip_path" 2>/dev/null || true
                    fi
                  fi
                done

                # Download images
                jq -r '.items[]?.image // empty' "/tmp/$store_index" | while read img_url; do
                  if [[ -n "$img_url" ]]; then
                    img_path=$(echo "$img_url" | sed "s|$BASE_URL/||")
                    mkdir -p "dist/$(dirname "$img_path")"
                    echo "  ‚¨áÔ∏è  IMG: $img_path"
                    curl -f -s "$img_url" -o "dist/$img_path" 2>/dev/null || true
                  fi
                done

                # Download manifests
                jq -r '.items[]?.manifest_url // empty' "/tmp/$store_index" | while read manifest_url; do
                  if [[ -n "$manifest_url" ]]; then
                    manifest_path=$(echo "$manifest_url" | sed "s|$BASE_URL/||")
                    mkdir -p "dist/$(dirname "$manifest_path")"
                    echo "  ‚¨áÔ∏è  MNF: $manifest_path"
                    curl -f -s "$manifest_url" -o "dist/$manifest_path" 2>/dev/null || true
                  fi
                done

                # Download examples (only if they don't exist locally)
                # Examples are cached by version: author:id@version
                # Format: dist/examples/author:id@version/index.html
                # If example_url exists in index, it means example was already built and published
                # We only restore index.html as a marker - pack_all_stores.sh checks for it
                # and skips rebuilding if it exists. If example files are incomplete,
                # the example will be rebuilt automatically.
                jq -r '.items[]?.example_url // empty' "/tmp/$store_index" | while read example_url; do
                  if [[ -n "$example_url" && "$example_url" != "null" ]]; then
                    if [[ "$example_url" != "$BASE_URL/"* ]]; then
                      echo "  ‚ÑπÔ∏è  EXM: external example URL detected ($example_url), skipping download"
                      continue
                    fi
                    # Extract example directory path from URL
                    # URL format: https://owner.github.io/repo/examples/author:id@version/index.html
                    example_path=$(echo "$example_url" | sed "s|$BASE_URL/||" | sed "s|/index.html$||")
                    if [[ -n "$example_path" ]]; then
                      # If example already exists locally, skip downloading
                      if [[ -f "dist/$example_path/index.html" ]]; then
                        echo "  ‚úÖ EXM: $example_path (already exists locally, skipping)"
                      else
                        # Create directory and download index.html as cache marker
                        # pack_all_stores.sh will check for index.html and skip rebuilding if it exists
                        mkdir -p "dist/$example_path"
                        if curl -f -s "$example_url" -o "dist/$example_path/index.html" 2>/dev/null; then
                          echo "  ‚úÖ EXM: $example_path (cache marker restored from GitHub Pages)"
                          echo "         Note: Example files will be restored from GitHub Pages on deploy"
                          echo "         or rebuilt by pack_all_stores.sh if incomplete"
                        else
                          echo "  ‚ÑπÔ∏è  EXM: $example_path (not found on GitHub Pages, will be built)"
                        fi
                      fi
                    fi
                  fi
                done
              fi
            done

            echo "‚úÖ Old assets restored"
          else
            echo "‚ÑπÔ∏è  No existing stores.json found, starting fresh"
          fi

      - name: pack all stores
        env:
          ASSETS_ROOT: .
          DIST_DIR: dist
          BASE_URL: ${{ steps.base.outputs.url }}
          GITHUB_OWNER: ${{ github.repository_owner }}
          GITHUB_REPO: ${{ github.event.repository.name }}
          GITHUB_BRANCH: main
        run: bash scripts/pack_all_stores.sh

      - name: configure pages
        uses: actions/configure-pages@v5

      - name: upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: dist

      - name: deploy to pages
        uses: actions/deploy-pages@v4

      - name: summary
        run: |
          echo "Published at: ${{ steps.base.outputs.url }}/"
