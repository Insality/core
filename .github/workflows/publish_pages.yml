name: publish_pages

on:
  push:
    branches: [ main, dev ]
  workflow_dispatch:
    inputs:
      clean_build:
        description: 'Remove restored assets and rebuild everything from scratch'
        type: boolean
        default: false

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: install deps
        run: |
          sudo apt-get update
          sudo apt-get install -y jq zip curl wget

      - name: setup java
        uses: actions/setup-java@v5
        with:
          distribution: 'zulu'
          java-version: '21'

      - name: compute base url
        id: base
        run: |
          OWNER="${GITHUB_REPOSITORY_OWNER}"
          REPO="$(echo "${GITHUB_REPOSITORY#*/}")"
          echo "url=https://${OWNER}.github.io/${REPO}" >> $GITHUB_OUTPUT

      - name: set clean build flag
        id: clean
        run: |
          if [[ "${GITHUB_EVENT_NAME}" == "workflow_dispatch" && "${CLEAN_BUILD_INPUT}" == "true" ]]; then
            echo "value=true" >> "$GITHUB_OUTPUT"
            echo "üßπ Clean build requested. Restored assets will be skipped."
          else
            echo "value=false" >> "$GITHUB_OUTPUT"
          fi
        env:
          CLEAN_BUILD_INPUT: ${{ github.event.inputs.clean_build || 'false' }}

      - name: clean dist directory
        if: steps.clean.outputs.value == 'true'
        run: |
          echo "üßΩ Removing existing dist directory for clean build"
          rm -rf dist

      - name: restore old assets
        if: steps.clean.outputs.value != 'true'
        continue-on-error: true
        run: |
          # Download existing ZIP files from GitHub Pages to preserve old versions
          BASE_URL="https://${GITHUB_REPOSITORY_OWNER}.github.io/$(echo ${GITHUB_REPOSITORY#*/})"
          echo "üì• Downloading existing assets from: $BASE_URL"
          mkdir -p dist

          # Try to download stores.json to get list of all stores
          if curl -f -s "$BASE_URL/stores.json" -o /tmp/old_stores.json 2>/dev/null; then
            echo "‚úÖ Found existing stores.json"

            # Read each store index and download ZIPs and images from it
            for store_index in $(jq -r '.stores[].index' /tmp/old_stores.json | sed 's|.*/||'); do
              echo "üìÇ Processing store: $store_index"

              if curl -f -s "$BASE_URL/$store_index" -o "/tmp/$store_index" 2>/dev/null; then
                # Download ZIPs (only if they don't exist locally)
                jq -r '.items[]?.zip_url // empty' "/tmp/$store_index" | while read zip_url; do
                  if [[ -n "$zip_url" ]]; then
                    zip_path=$(echo "$zip_url" | sed "s|$BASE_URL/||")
                    if [[ -f "dist/$zip_path" ]]; then
                      echo "  ‚úÖ ZIP: $zip_path (already exists, skipping)"
                    else
                      mkdir -p "dist/$(dirname "$zip_path")"
                      echo "  ‚¨áÔ∏è  ZIP: $zip_path"
                      curl -f -s "$zip_url" -o "dist/$zip_path" 2>/dev/null || true
                    fi
                  fi
                done

                # Download images
                jq -r '.items[]?.image // empty' "/tmp/$store_index" | while read img_url; do
                  if [[ -n "$img_url" ]]; then
                    img_path=$(echo "$img_url" | sed "s|$BASE_URL/||")
                    mkdir -p "dist/$(dirname "$img_path")"
                    echo "  ‚¨áÔ∏è  IMG: $img_path"
                    curl -f -s "$img_url" -o "dist/$img_path" 2>/dev/null || true
                  fi
                done

                # Download manifests
                jq -r '.items[]?.manifest_url // empty' "/tmp/$store_index" | while read manifest_url; do
                  if [[ -n "$manifest_url" ]]; then
                    manifest_path=$(echo "$manifest_url" | sed "s|$BASE_URL/||")
                    mkdir -p "dist/$(dirname "$manifest_path")"
                    echo "  ‚¨áÔ∏è  MNF: $manifest_path"
                    curl -f -s "$manifest_url" -o "dist/$manifest_path" 2>/dev/null || true
                  fi
                done

                # Download examples (only if they don't exist locally)
                # Examples are cached by version: author:id@version
                # Format: dist/examples/author:id@version/index.html
                # Collect all example URLs to restore all versions (including old ones)
                jq -r '.items[]?.example_url // empty' "/tmp/$store_index" >> /tmp/all_example_urls.txt 2>/dev/null || true
              fi
            done

            # Restore all examples from collected URLs (including old versions)
            if [[ -f /tmp/all_example_urls.txt ]]; then
              echo "üì¶ Restoring examples from all versions..."
              sort -u /tmp/all_example_urls.txt | while read example_url; do
                if [[ -n "$example_url" && "$example_url" != "null" ]]; then
                  if [[ "$example_url" != "$BASE_URL/"* ]]; then
                    echo "  ‚ÑπÔ∏è  EXM: external example URL detected ($example_url), skipping download"
                    continue
                  fi
                  # Extract example directory path from URL
                  # URL format: https://owner.github.io/repo/examples/author:id@version/index.html
                  example_path=$(echo "$example_url" | sed "s|$BASE_URL/||" | sed "s|/index.html$||")
                  if [[ -n "$example_path" ]]; then
                    # If example directory already exists locally, skip downloading
                    if [[ -d "dist/$example_path" ]]; then
                      echo "  ‚úÖ EXM: $example_path (already exists locally, skipping)"
                    else
                      # Download all known files from example directory
                      # GitHub Pages doesn't support directory listing, so we download known files directly
                      echo "  ‚¨áÔ∏è  EXM: Downloading $example_path from GitHub Pages..."
                      example_base_url=$(echo "$example_url" | sed "s|/index.html$||")
                      mkdir -p "dist/$example_path"
                      
                      # Download main files
                      downloaded_count=0
                      for file in index.html dmloader.js Core.wasm Core_wasm.js; do
                        if curl -f -s "$example_base_url/$file" -o "dist/$example_path/$file" 2>/dev/null; then
                          downloaded_count=$((downloaded_count + 1))
                        fi
                      done
                      
                      # Download archive folder files
                      mkdir -p "dist/$example_path/archive"
                      for file in game0.public.der game0.dmanifest game0.arci game0.arcd game0.projectc archive_files.json; do
                        curl -f -s "$example_base_url/archive/$file" -o "dist/$example_path/archive/$file" 2>/dev/null || true
                      done
                      
                      if [[ $downloaded_count -gt 0 ]]; then
                        echo "  ‚úÖ EXM: $example_path (restored $downloaded_count file(s) from GitHub Pages)"
                      else
                        echo "  ‚ö†Ô∏è  EXM: $example_path (not found on GitHub Pages, will be rebuilt if needed)"
                      fi
                    fi
                  fi
                fi
              done
              rm -f /tmp/all_example_urls.txt
            fi

            echo "‚úÖ Old assets restored"
          else
            echo "‚ÑπÔ∏è  No existing stores.json found, starting fresh"
          fi

      - name: pack all stores
        env:
          ASSETS_ROOT: .
          DIST_DIR: dist
          BASE_URL: ${{ steps.base.outputs.url }}
          GITHUB_OWNER: ${{ github.repository_owner }}
          GITHUB_REPO: ${{ github.event.repository.name }}
          GITHUB_BRANCH: main
        run: bash scripts/pack_all_stores.sh

      - name: configure pages
        uses: actions/configure-pages@v5

      - name: upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: dist

      - name: deploy to pages
        uses: actions/deploy-pages@v4

      - name: summary
        run: |
          echo "Published at: ${{ steps.base.outputs.url }}/"
